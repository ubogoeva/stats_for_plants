---
title: "R и статистика в селекции: лекция 6"
subtitle: 'Корреляция и линейная регрессия'
author: "Elena U"
#date: "Created on 01 April, 2023"
execute:
  echo: true
  output: true
format: 
  revealjs:
    slide-number: c/t
    show-slide-number: all
    # mainfont: Arial
    # fontsize: 14px
    theme: [default, custom.scss]
    chalkboard: 
      buttons: true
    # theme: [serif]
    # mouse-wheel: true
    auto-play-media: true
    width: 1280
    height: 720
    fig-dpi: 300
    # logo: figures/icg.png
revealjs-plugins:
  - pointer
css: styles.css
editor: visual
filters: [bg_style.lua]
---

## План лекции

-   Ковариация и корреляция

-   Простая линейная регрессия: формула, реализация в R, интерпретация

-   Ограничения линейной регрессии (в т.ч. нормальность распределения остатков)

-   Множественная линейная регрессия

# Ковариация и корреляция

## Данные для работы

Несколько сортов шпината, урожайность и дни до сбора урожая.

```{r}
library(tidyverse)
df <- read_csv2('data/Spin.data.csv')
head(df)
str(df)
```

## Посмотрим на данные

Используя базовую generic-функцию `summary()`.

```{r}
summary(df)
```

Будем исследовать, связаны ли дни и урожайность.

## Посмотрим на данные

С помощью функции `skim()` из пакета `skimr`.

```{r}
skim_res <- skimr::skim(df)
knitr::kable(skim_res)
```

Это запись для презентации, обычно достаточно запустить:

```{r}
#| eval: false
# install.packages('skimr')
library(skimr)
skim(df)
```

## График зависимости 

```{r}
#| fig-height: 4
#| fig-width: 5
plot(df$Days, df$Yield)
```

## Ковариация

Самая простая мера связи между двумя количественными переменными --- это ковариация. Если ковариация положительная, то чем больше одна переменная, тем больше другая переменная. При отрицательной ковариации наоборот: чем больше одна переменная, тем меньше другая.

Формула ковариации:

$$ \sigma_{xy} = cov(x, y) = \frac{\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{n} $$

Оценка ковариации по выборке:

$$ \hat{\sigma}_{xy} = \frac{\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{n-1} $$

Если оценить ковариацию переменной с самой собой, какая будет формула?

## Ковариация {style="font-size: 90%"}

Ковариация переменной с самой собой - это дисперсия.

Давайте посчитаем ковариацию: функция `cov()`.

```{r}
cov(df[,2:3])
```

По главной диагонали ковариация переменной с самой собой - дисперсия.

Для подсчета ковариации для двух величин:

```{r}
cov(df$Days, df$Yield)
```

Можем ли мы сделать вывод, это относительно большая ковариация между переменными или нет?

## Коэффициент корреляции {style="font-size: 90%"}

Проблема ковариации в том, что она привязана к исходной шкале и измеряется в пределах $[-\infty; \infty]$. Неплохо было бы иметь возможность нормировать. Для этого используется коэффициент корреляции.

Формула коэффициента корреляции Пирсона:

$$ \rho_{xy} = \frac{\sigma_{xy}}{\sigma_x \sigma_y} = \frac{\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\sum_{i = 1}^n(x_i - \overline{x})^2}\sqrt{\sum_{i = 1}^n(y_i - \overline{y})^2}} = \frac{1}{n}\sum_{i = 1}^n z_{x,i} z_{y, i} $$

Оценка коэффициента корреляции Пирсона по выборке:

$$ r_{xy} = \frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x \hat{\sigma}_y} = \frac{\sum_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\sum_{i = 1}^n(x_i - \overline{x})^2}\sqrt{\sum_{i = 1}^n(y_i - \overline{y})^2}} = \frac{1}{n - 1}\sum_{i = 1}^n z_{x,i} z_{y, i} $$

По сути это ковариация, деленная на стандартное отклонение обеих переменных.

Коэффициент корреляции измеряется в пределах $[-1; 1]$.

## Вычисляем корреляцию

Используем функцию `cor()` базового R.

```{r}
cor(df$Days, df$Yield)
```

Посмотрим визуально на распределение данных.

```{r}
#| fig-height: 4
#| fig-width: 5
plot(df$Days, df$Yield)
```

Значим ли этот коэффициент корреляции?

## Алгоритм статистического вывода

1.  Формулировка нулевой и альтернативной гипотезы.

2.  Вычисление тестовых статистик.

3.  Подсчет p-value как площади под кривой выборочного распределения тестовых статистик.

4.  Вывод: отклоняем или не отклоняем нулевую гипотезу.

## Тестируем значимость коэффициента корреляции

Нулевая гипотеза: коэффициент корреляции равен нулю\
Альтернативная: коэффициент корреляции не равен нулю.

В качестве тестовой статистики используется t-статистика. Для расчета p-value используется t-распределение с $n_1+n_2-2$ степенями свободы, где n1, n2 -- размер выборки 1 и 2.

Мы это вручную считать не будем, воспользуемся встроенной функцией `cor.test()`.

## Расчет с помощью функции

```{r}
cor.test(df$Days, df$Yield)
```

По умолчанию рассчитывается коэффициент корреляции Пирсона: оценивает связь двух нормально распределенных величин. Выявляет только линейную составляющую взаимосвязи.

## Непараметрические аналоги коэффициента корреляции Пирсона {style="font-size: 80%"}

Часто используется коэффициент корреляции Спирмена (Spearman), если в данных есть выбросы. Математика метода точно такая же как у коэффициента Пирсона, только вместо оригинальных значений используются ранги.

В целом, непараметрические критерии не зависят от формы распределения и могут оценивать связь для любых монотонных зависимостей.

```{r}
cor.test(df$Days, df$Yield, method = 'spearman')
```

::: {.callout-warning appearance="simple"}
Наличие значимого коэффициента корреляции ничего не говорит о причинно-следственной связи между переменными!
:::

## Непараметрические аналоги коэффициента корреляции Пирсона

Также есть коэффициент корреляции Кендалла.

```{r}
cor.test(df$Days, df$Yield, method = 'kendall')
```

## Сравнение различных коэффициентов корреляции

| Пирсона                               | Спирмена                                                            | Кендалла                              |
|--------------------|---------------------------------|--------------------|
| Выявляет линейную зависимость         | Выявляет любую монотонную зависимость                               | Выявляет любую монотонную зависимость |
| Количественная или интервальная шкала | шкала \>= ранговая (то есть ранговая, интервальная, количественная) | шкала \>= ранговая                    |
| Неустойчивость к выбросам             | Устойчивость к выбросам                                             | Устойчивость к выбросам               |

------------------------------------------------------------------------

Пример ситуации, когда коэффициент Спирмена более применим:

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 6
set.seed(2)
x <- rnorm(n = 20, mean = 4, sd = 4) + rnorm(20, mean = 10, sd = 4)
# x <- runif(n = 20, min = 2, max = 22)

y <- (x^6.3) + rnorm(20) 
ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) + 
  geom_point(size = 3)+
  theme_bw()+
  ggtitle(label = paste0("p-value of Pearson's correlation coefficient is: ",  
                         formatC(cor.test(x,y)$p.value, 2)),
          subtitle = paste0("p-value of Spearman's correlation coefficient is: ",  
                         formatC(cor.test(x,y, method = 'spearman')$p.value, 2)))+
  theme(plot.title = element_text(size = 20),
        plot.subtitle = element_text(size = 20))

```

Проведя корреляционный анализ, мы лишь ответили на вопрос "Существует ли статистически значимая связь между величинами?"

Сможем ли мы, используя это знание, **предсказать** значения одной величины, исходя из знаний другой?

# Линейная регрессия

## Данные для работы

```{r}
library(readxl)
df_regr <- read_xls('data/QM-mod7-ex2data.xls') %>% 
  janitor::clean_names()
head(df_regr)
```

## Какие бывают регрессионные модели?

::: incremental
-   Линейные и нелинейные

    -   Линейные

        $$ y = b_0 + b_1x $$ $$ y = b_0 + b_1x_1 + b_2x_2 $$

        $$ y = b_0 + b_1x_1^2 + b_2x_2^3 $$

    -   Нелинейные

        $$ y = b_0 + b_1^x $$ $$ y = b_0^{b_1x_1+b_2x_2} $$

-   Простые и множественные
:::

## Линейная регрессия: формула

Линейная регрессия позволяет предсказать значение одной переменной на основании значений другой. Обе переменные должны быть количественными.

Формула для простой линейной регрессии - по сути это формула прямой из алгебры + остатки:

$$ Y = ax + b + \epsilon $$

$Y$ - зависимая переменная, что пытаемся предсказать, $\epsilon$ - это ошибки или остатки модели (residuals) - то есть то, что наша модель не смогла предсказать.

Для более сложных моделей нотация немного меняется:

$$ Y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon $$

## Линейная регрессия

В R функция для линейной регрессии - `lm()`. Давайте попробуем предсказать значение урожайности `corn_yield` на основании водного стресса (`water_stress`).

```{r}
model <- lm(corn_yield ~ water_stress, data = df_regr)
model
```

Мы получили коэффициенты: Intercept, water_stress. Что это значит?

Как эти коэффициенты подбираются?

::: incremental
-   Метод наименьших квадратов (для простых моделей) -\> нарисовать

-   Метод максимального правдоподобия (для более сложных)
:::

## График регрессионной прямой на основании полученных коэффициентов

corn_yield = Intercept + water_stress \* Slope = 9.7761 + water_stress \*(-0.1167) = 9.7761 - water_stress \*0.1167

Попробуем нарисовать регрессионную прямую на нашем графике.

```{r}
#| fig-height: 4
#| fig-width: 5
#| output-location: column
ggplot(df_regr, aes(water_stress, corn_yield))+   
  geom_point(alpha = 0.8)+  
  geom_abline(slope = model$coefficients[2],
              intercept = model$coefficients[1], 
              color = 'blue')+
  theme_bw()
```

## Предсказание значений на основе регрессионной прямой {style="font-size: 90%"}

Функция `predict()` принимает на вход независимую переменную и предсказывает значение зависимой на основании нашей модели (подставляет в формулу).

Например, предскажем `Yield` для `water_stress` = 30. (Нужно подать именно датафрейм).

```{r}
predict(model, data.frame(water_stress = 30))
```

Функция `predict()` ничего не знает о физическом смысле наших данных, поэтому мы можем предсказывать любые, даже неадекватные значения, например

```{r}
predict(model, data.frame(water_stress = -5))
```

::: {.callout-important appearance="simple"}
Предсказания регрессионной модели имеют смысл **только** "внутри" графика!

Это называется интерполяция, а экстраполяция почти всегда будет неверной.
:::

## Экстраполяция и интерполяция

```{r}
#| echo: false
ggplot(df_regr, aes(water_stress, corn_yield))+
  geom_point(alpha = 0.8)+
  geom_abline(slope = model$coefficients[2], 
              intercept = model$coefficients[1], color = 'blue')+
  scale_x_continuous(limits = c(-10, 80), 
                     breaks = seq(0, 80, 5),
                     expand = c(0,0))+
  geom_vline(xintercept = min(df_regr$water_stress), 
             linetype = 2)+
  geom_vline(xintercept = max(df_regr$water_stress), 
             linetype = 2)+
  annotate('text', x = -2, y = 3, label = 'Extrapolation',
           angle = 90)+
  annotate('text', x = 76, y = 3, label = 'Extrapolation',
           angle = 90)+
  annotate('text', x = 30, y = 3.2, label = 'Interpolation')+
  theme(axis.text = element_text(size = 14),
        axis.title = element_text(size = 16))+
  theme_minimal()
```

## Более внимательно посмотрим на вывод регрессии

```{r}
summary(model)
```

Что все это значит?

## Самое важное из вывода регрессии

```{r}
summary(model)$coefficients
```

p-value есть для каждого коэффициента,

$H_0: Intercept = 0, Slope = 0$.

$H_1: Intercept \neq 0, Slope \neq 0$.

Какое значение p-value для нас важнее?

::: fragment
Коэффициент наклона.
:::

Как рассчитывается значимость коэффициента наклона?

::: fragment
Рассчитывается относительно "нулевой" модели - то есть модели, в которую входит только интерсепт.
:::

## Коэффициент детерминации - мера качества модели

$R^2$ - коэффициент детерминации. Описывает какую долю дисперсии зависимой переменной объясняет модель.

$$ R^2 = \frac{\color{blue}{SS_{r}}}{SS_{t}} $$

-   Измеряется от $[0, 1]$.

-   $R^2 = r^2$ -- для простой линейной регрессии коэффициент детерминации -- квадрат коэффициента корреляции Пирсона.

## Допущения линейной регрессии

-   Линейная связь

-   Независимость

-   Гомогенность дисперсий

-   Отсутствие коллинеарности предикторов (для множественной регрессии)

-   Нормальное распределение ошибок -\> проиллюстрировать другие графики в R

    ```{r}
    #| eval: false
    plot(model)
    ```

## Линейность связи

Нелинейность связи видно на графиках остатков.

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 6
library(patchwork)
set.seed(39484)
x <- rnorm(100, 10, 3)
y <- (x^2.4) + rnorm(100, 0, 100)
pl_1 <- ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) + 
  geom_point()+theme_bw()
lm1 <- lm(y ~ x)
pl_1res <- ggplot(data.frame(fit = fitted(lm1), 
                             res = residuals(lm1)), 
                  aes(x = fit, y = res)) + 
  geom_point() + 
  geom_hline(yintercept = 0) + 
  xlab("Fitted") + 
  ylab("Residuals")+theme_bw()
x2 <- runif(100, 1, 8)
y2 <- sin(x2) + 2 * x2 + rnorm(100)
pl_2 <- ggplot(data.frame(x = x2, y = y2), aes(x = x, y = y)) +
        geom_point() +theme_bw()
lm2 <- lm(y2 ~ x2)
pl_2res <- ggplot(data.frame(fit = fitted(lm2), 
                             res = residuals(lm2)), 
                             aes(x = fit, y = res)) + 
  geom_point() + 
  geom_hline(yintercept = 0) + 
  xlab("Fitted") + 
  ylab("Residuals")+theme_bw() 
(pl_1 / pl_1res) | (pl_2 / pl_2res)
```
:::

::: {.column width="50%"}
::: columns
Проверка на линейность связи:

-   График зависимости y от x (и от других переменных, не включенных в модель).

-   График остатков от предсказанных значений.

Что делать с нелинейностью данных?

-   Добавить неучтенные переменные или взаимодействия

-   Применить линеаризующее преобразование (Осторожно!)

-   Применить обобщенную линейную модель с другой функцией связи (GLM)
:::
:::
:::

## График остатков для наших данных

```{r}
#| output-location: slide 
ggplot(data.frame(fit = fitted(model),                               res = residuals(model)), aes(x = fit, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = 2) +
  xlab("Fitted") +
  ylab("Residuals")+   
  theme_bw()
```

## Независимость данных

Каждое значение $y_i$ должно быть независимо от любого другого $y_j$.

Это возможно проконтролировать на этапе сбора данных

Наиболее частые источники зависимостей:

-   Псевдоповторности (когда измерили один и тот же объект несколько раз)

-   Временные и пространственные автокорреляции

-   Неучтенные переменные

## Нормальное распределение ошибок

Можно проверить с помощью `qqPlot()`.

```{r}
car::qqPlot(model$residuals)
```

## Проверка на гомогенность дисперсий (гомоскедастичность)

Многие тесты чувствительны к гетероскедастичности.

Лучший способ проверки на гомогенность дисперсий --- график остатков от предсказанных значений.

```{r}
#| output-location: slide 
ggplot(data.frame(fit = fitted(model),                               res = residuals(model)), aes(x = fit, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = 2) +
  xlab("Fitted") +
  ylab("Residuals")+   
  theme_bw()
```

## Проверка на гомогенность дисперсий

![взято из презентации Марины Варфоломеевой](images/image-1051634690.png){alt="взято из презентации Марины Варфоломеевой"}
