---
title: "R и статистика в селекции: лекция 7"
subtitle: 'Множественная линейная регрессия'
author: "Elena U"
#date: "Created on 01 April, 2023"
execute:
  echo: true
  output: true
format: 
  revealjs:
    slide-number: c/t
    show-slide-number: all
    # mainfont: Arial
    # fontsize: 14px
    theme: [default, custom.scss]
    chalkboard: 
      buttons: true
    # theme: [serif]
    # mouse-wheel: true
    auto-play-media: true
    width: 1280
    height: 720
    fig-dpi: 300
    # logo: figures/icg.png
revealjs-plugins:
  - pointer
css: styles.css
editor: visual
filters: [bg_style.lua]
---

```{r}
#| include: false
library(tidyverse)
```

## План лекции

-   Множественная линейная регрессия

-   Коэффициент детерминации

-   Однофакторный дисперсионный анализ

## Формулировка множественной линейной регрессии

$$ Y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon $$

Интерпретация коэффициентов:

**Intercept** (интерсепт, перехват) -- значение зависимой переменной, когда все независимые равны нулю.

**Коэффициент перед предиктором**: показывает на сколько единиц изменится значение зависимой переменной в случае, если значение **этого** предиктора изменится на единицу, а все **другие** показатели не изменятся.

## Геометрический смысл

Для двух коэффициентов теперь фитим (подгоняем) плоскость в трехмерном пространстве.

$Y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon$

![](https://miro.medium.com/v2/resize:fit:597/1*RqL8NLlCpcTIzBcsB-3e7A.png)

Для большего количества предикторов нарисовать нет возможности, но смысл остается тот же самый.

## Нотация формул линейных моделей в R

+-----------------------------------------------------------+---------------+
| Модель                                                    | Формула       |
+===========================================================+===============+
| Простая линейная регрессия                                | Y \~ X        |
+-----------------------------------------------------------+---------------+
| Простая линейная регрессия, без интерсепта b~0~           | Y \~ -1 + X   |
+-----------------------------------------------------------+---------------+
| Уменьшенная простая линейная регрессия (только интерсепт) | Y \~ 1 \      |
|                                                           | Y \~ 1 - X    |
+-----------------------------------------------------------+---------------+
| Множественная линейная регрессия                          | Y \~ X1 + X2  |
+-----------------------------------------------------------+---------------+

## Нотация формул линейных моделей в R

+-----------------+---------------------------------------------------------------------------+
| Элемент формулы | Значение                                                                  |
+=================+===========================================================================+
| :               | Взаимодействие предикторов\                                               |
|                 | Y \~ X1 + X2 + X1:X2                                                      |
+-----------------+---------------------------------------------------------------------------+
| \*              | Все возможные взаимодействия                                              |
|                 |                                                                           |
|                 | Y \~ X1\*X2 тоже самое, что и\                                            |
|                 | Y \~ X1+X2 + X1:X2                                                        |
+-----------------+---------------------------------------------------------------------------+
| .               | Y \~ .                                                                    |
|                 |                                                                           |
|                 | В правой части формулы записываются все переменные из датафрейма, кроме Y |
+-----------------+---------------------------------------------------------------------------+

## Условия применимости множественной линейной регрессии {style="font-size: 90%"}

1.  Линейная связь (=отсутствие паттерна на графике остатков).

2.  Независимость наблюдений.

3.  Гомогенность дисперсий.

4.  Нормально распределение ошибок (остатков).

5.  Отсутствие влиятельных наблюдений.

6.  **Отсутствие коллинеарности предикторов (для множественной регрессии)**

Все условия совпадают с требованиями к простой линейной регрессии + проверка на коллинеарность.

Вообще перечисленные выше условия применимости совпадают и с многими другими статистическими тестами (и это не случайно!)

## Данные для работы с регрессией

```{r}
library(tidyverse)
df_maize <- read_csv('data/ex3.csv')
head(df_maize)

```

Зависимая переменная -- урожайность (`yield`), независимые переменные -- степень засухи от -4 до 4 (`drought`), количество удобрения (`N`), реплика и линия.

## Посмотрим на данные

```{r}
#| output-location: column
#| fig-width: 6
#| fig-height: 4
ggplot(df_maize, aes(N, yield))+
  geom_point()
```

## Добавим переменную засухи и линию регрессии

```{r}
#| output-location: column
#| fig-width: 6
#| fig-height: 4
ggplot(df_maize, aes(N, yield))+
  geom_point(aes(color = drought))+
  geom_smooth()
```

## Проверим корреляцию каждой переменной с каждой

С помощью функции `cor()`. Но мы не получаем p-value для тестов.

```{r}
cor(df_maize)
```

Функция `cor.test()` дает p-value только для одной пары переменных.

## Проверим корреляцию каждой переменной с каждой

```{r}
library(psych)
psych::corr.test(df_maize[,1:3])
```

По умолчанию используется поправка Холма на множественные сравнения.

## Визуализация корреляций

```{r}
library(corrplot)
corrplot(cor(df_maize))
```

## Коэффициент детерминации: формула

Обозначается как $R^2$ -- доля объясненной дисперсии.

$$ R^2 = \frac{SSR}{SST} $$

$SSR = \sum_i{(\hat{y_i}-\overline{y})^2}$ - объясненная моделью дисперсия.

$SSE = \sum{(y_i - \hat{y_i})^2}$ - необъясненная дисперсия.

$SST = SSR + SSE = \sum_i{(y_i-\overline{y})^2}$ - общая дисперсия.

Объясненную моделью дисперсию можно вычислить как разницу между общей дисперсией и необъясненной дисперсией ($SSR = SST - SSE$).

Разберем на рисунке:

## Коэффициент детерминации: график

```{r}
#| echo: false
library(patchwork)
df_test <- data.frame(
  x=c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
  y=c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3))
model <- lm(y ~ x, data = df_test)
df_mean <- df_test %>% 
  mutate(y1 = mean(y), x1 = x)
  # rename(x = x2)
df_regr <- df_test %>% 
  mutate(y1 = predict(model, data.frame(x = df_test$x)), x1 = x)
# predict(model, data.frame(x = df$x))
regr_plot <- df_test %>% 
  ggplot(aes(x, y))+
  geom_point()+
  geom_smooth(formula = 'y ~ x', method = 'lm', se = FALSE, linewidth = 0.7)+
  geom_segment(aes(x = x1, y = y1, xend = x, yend = y), 
               data = df_regr, linewidth = 0.4, linetype = 2)+
  ggtitle('Residual variation SSE')+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5))

# regr_plot
mean_plot <- df_test %>% 
  ggplot(aes(x, y))+
  geom_point()+
  geom_hline(yintercept = mean(df_test$y), color = 'blue', linewidth = 0.7)+
  geom_segment(aes(x = x1, y = y1, xend = x, yend = y), 
               data = df_mean, linewidth = 0.4, linetype = 2)+
  annotate('text', x = 1.2, y = mean(df_test$y)+0.2, label = 'y_mean')+
  ggtitle('Total variation (SST)')+
  # geom_smooth(formula = 'y ~ x', method = 'lm', se = FALSE)+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5))
mean_plot + regr_plot
```

## Коэффициент детерминации: расчет

```{r}
df_test <- data.frame(
  x=c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
  y=c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3))
sst <- sum((df_test$y - mean(df_test$y))^2) # общая сумма квадратов, SST
ssr <- sst - sum((df_test$y - predict(model, data.frame(x = df_test$x)))^2)
r2 <- ssr / sst
r2
```

```{r}
#| output-location: column
summary(lm(y ~ x, data = df_test)) # все сходится
```

## Коэффициент детерминации

```{r}
#| echo: false
df_test %>% 
  ggplot(aes(x, y))+
  geom_point()+
  geom_smooth(formula = 'y ~ x', method = 'lm', 
              se = FALSE, linewidth = 0.7, color = 'gray50')+
  geom_hline(yintercept = mean(df_test$y), color = 'red', linewidth = 0.5)+
  geom_segment(aes(x = x1, y = y, xend = x, yend = mean(y)), 
               data = df_regr, linewidth = 0.4, 
               linetype = 1, color = '#253494', alpha = 0.8)+
  geom_segment(aes(x = x1, y = y1, xend = x, yend = y), 
               data = df_regr, linewidth = 0.4, 
               linetype = 2, color= '#41b6c4')+
  # ggtitle('Total variation')+
  annotate('text', x = 6.1, y = 4.5, label = 'SSR', color= '#253494')+
  annotate('text', x = 6.1, y = 5.8, label = 'SSE', color= '#41b6c4')+
  geom_segment(aes(x = max(x), y = mean(y),
                   xend = max(x)+0.1, yend = mean(y)+0.1))+
  geom_segment(aes(x = max(x), y = max(y),
                   xend = max(x)+0.1, yend = max(y)-0.1))+
  geom_segment(aes(x = max(x)+0.1, y = mean(y)+0.1,
                   xend = max(x)+0.1, yend = max(y)-0.1))+
  annotate('text', x = 6.6, y = 5, label = 'SST')+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5))
```

## Интерпретация коэффициента детерминации

![](images/image-1678901130.png)

## Множественная линейная регрессия

Можно использовать несколько предикторов для объяснения зависимой переменной.

Больше предикторов - лучше модель?

```{r}
model_full <- lm(yield ~ drought + N + line, df_maize)
summary(model_full)
```

## Проблема мультиколлинеарности во множественной линейной регрессии {style="font-size: 80%"}

Мультиколлинеарность --- наличие линейной зависимости между независимыми переменными в регрессионной модели.

При наличии мультиколлинеарности оценки параметров неточны, увеличиваются стандартные ошибки, а значит сложно интерпретировать влияние предикторов на отклик.

Для множественной линейной регрессии очень плохо, когда независимые переменные коррелируют друг с другом. Чтобы оценить, насколько у нас выражена автокорреляция, построим еще раз плот корреляций (пакет `corrplot`, функция `corrplot()`).

```{r}
corrplot(cor(df_maize[,1:4]))
```

Автокорреляция выражена не очень сильно.

::: callout-tip
## Совет

Если есть проблема с мультиколлинеарностью, можно посчитать VIF, подробнее спросить у лектора
:::

## Дисперсионный анализ (ANOVA)

Дисперсионный анализ (analysis of variance, ANOVA) --- метод для сравнения средних в трех и более группах.

Виды дисперсионного анализа:

-   Однофакторный (one-way)

-   Двухфакторный (two-way)

-   MANOVA (Multivariate analysis of variance)

-   ANCOVA

Мы будем подробно рассматривать первые два.

# Однофакторный дисперсионный анализ {#one_way_anova}

## Данные для работы

```{r}
library(readxl)
df_aov <- read_xlsx('data/CRD.1.data.xlsx')
df_aov
```

## Однофакторный дисперсионный анализ: формула

+-----------------+------------------+---------------------------+---------------------------------+---------------------------+
|                 | Степени свободы  | Суммы квадратов           | Средние квадраты                | F-статистика              |
+=================+==================+===========================+=================================+===========================+
| Межгрупповые    | $df_{b} = J - 1$ | $SS_{b}$                  | $MS_{b} =\frac{SS_{b}}{df_{b}}$ | $F=\frac{MS_{b}}{MS_{w}}$ |
+-----------------+------------------+---------------------------+---------------------------------+---------------------------+
| Внутригрупповые | $df_{w} = N - J$ | $SS_{w}$                  | $MS_{w} =\frac{SS_{w}}{df_{w}}$ |                           |
+-----------------+------------------+---------------------------+---------------------------------+---------------------------+
| Общие           | $df_{t} = N - 1$ | $SS_{t}= SS_{b} + SS_{w}$ |                                 |                           |
+-----------------+------------------+---------------------------+---------------------------------+---------------------------+

$J$ означает количество групп, $N$ - общее количество наблюдений во всех группах.

## Однофакторный дисперсионный анализ: формула

+-----------------+------------------+---------------------------------------------------------------------------------------+---------------------------------+---------------------------+
|                 | Степени свободы  | Суммы квадратов                                                                       | Средние квадраты                | F-статистика              |
+=================+==================+=======================================================================================+=================================+===========================+
| Межгрупповые    | $df_{b} = J - 1$ | $SS_{b}= \sum\limits_{j=1}^J \sum\limits_{i=1}^{n_j} (\overline{x_j}-\overline{x})^2$ | $MS_{b} =\frac{SS_{b}}{df_{b}}$ | $F=\frac{MS_{b}}{MS_{w}}$ |
+-----------------+------------------+---------------------------------------------------------------------------------------+---------------------------------+---------------------------+
| Внутригрупповые | $df_{w} = N - J$ | $SS_{w}= \sum\limits_{j=1}^J \sum\limits_{i=1}^{n_j} (x_{ij}-\overline{x_j})^2$       | $MS_{w} =\frac{SS_{w}}{df_{w}}$ |                           |
+-----------------+------------------+---------------------------------------------------------------------------------------+---------------------------------+---------------------------+
| Общие           | $df_{t} = N - 1$ | $SS_{t}= \sum\limits_{j=1}^J \sum\limits_{i=1}^{n_j} (x_{ij}-\overline{x})^2$         |                                 |                           |
+-----------------+------------------+---------------------------------------------------------------------------------------+---------------------------------+---------------------------+

$n_j$ означает количество наблюдений в группе $j$, а $x_{ij}$ - наблюдение под номером $i$ в группе $j$.

## Посчитаем p-value

```{r}
df_aov$Pop <- as.factor(df_aov$Pop)
fit1 <- aov(Yield ~ Pop, df_aov)
summary(fit1)
```

## Ограничения ANOVA

-   Независимость наблюдений.

-   Нормальность распределения исходных данных - под вопросом (тоже самое, что и про t-test).

-   Равенство дисперсий исходных данных - необязательно, если дизайн сбалансированный.

-   **Нормальность распределения остатков** - очень важно.

-   **Равенство дисперсии распределения остатков**
